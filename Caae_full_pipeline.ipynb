{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtPJRV1EBjA2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import logging\n",
        "\n",
        "from pathlib import Path\n",
        "import logging\n",
        "import pandas as pd\n",
        "\n",
        "def clean_and_save_csv(csv_file: str) -> str:\n",
        "    \"\"\"\n",
        "    מנקה קובץ CSV: מסיר עמודת אינדקס, מפצל ומרפד את עמודת ה-Data, ושומר מחדש.\n",
        "    Label תמיד תופיע אחרי DATA[7].\n",
        "\n",
        "    Args:\n",
        "        csv_file (str): הנתיב לקובץ המקורי.\n",
        "\n",
        "    Returns:\n",
        "        str: הנתיב לקובץ החדש שנשמר.\n",
        "    \"\"\"\n",
        "    # קריאה עם עמודה אחת ל־Data\n",
        "\n",
        "    cols = ['Timestamp', 'CAN ID', 'DLC', 'Data', 'Label']\n",
        "    df = pd.read_csv(csv_file, header=None, names=cols, low_memory=False)\n",
        "\n",
        "    # הסרת עמודת index\n",
        "\n",
        "    # המרות בסיסיות\n",
        "    df['Timestamp'] = pd.to_numeric(df['Timestamp'], errors='coerce')\n",
        "    df['DLC'] = pd.to_numeric(df['DLC'], errors='coerce').fillna(0).astype(int)\n",
        "    df = df.dropna(subset=['Timestamp', 'CAN ID'])\n",
        "\n",
        "    # ניקוי ערכי Data\n",
        "    def process_data_field(raw):\n",
        "        if pd.isna(raw): return ['00'] * 8\n",
        "        raw = str(raw).replace(\" \", \"\")  # הסרת רווחים\n",
        "        bytes_list = [raw[i:i+2] for i in range(0, len(raw), 2)]\n",
        "        while len(bytes_list) < 8:\n",
        "            bytes_list.append(\"00\")\n",
        "        return bytes_list[:8]\n",
        "\n",
        "    # החלת הפיצול והפירוק\n",
        "    processed_data = df['Data'].apply(process_data_field)\n",
        "    data_df = pd.DataFrame(processed_data.tolist(), columns=[f'DATA[{i}]' for i in range(8)])\n",
        "    df.drop(columns=['Data'], inplace=True)\n",
        "\n",
        "    # עדכון DLC ל-8 אם היה פחות\n",
        "    df['DLC'] = 8\n",
        "\n",
        "    # בניית הסדר הנכון מחדש\n",
        "    final_df = pd.concat([df[['Timestamp', 'CAN ID', 'DLC']].reset_index(drop=True),\n",
        "                          data_df.reset_index(drop=True),\n",
        "                          df[['Label']].reset_index(drop=True)], axis=1)\n",
        "\n",
        "    # שמירה\n",
        "    out_file = Path(csv_file)\n",
        "    final_df.to_csv(out_file, index=False)\n",
        "    logging.info(f\"[CLEAN] Saved cleaned file: {out_file}\")\n",
        "    return str(out_file)\n",
        "\n",
        "# רשימת שמות הקבצים מתוך התמונה\n",
        "csv_files = [\n",
        "    \"Attack_free_CHEVROLET_Spark_train.csv\",\n",
        "    \"Attack_free_KIA_Soul_train.csv\",\n",
        "    \"Flooding_CHEVROLET_Spark_train.csv\",\n",
        "    \"Flooding_HYUNDAI_Sonata_train.csv\",\n",
        "    \"Flooding_KIA_Soul_train.csv\",\n",
        "    \"Fuzzy_CHEVROLET_Spark_train.csv\",\n",
        "    'Attack_free_HY_Sonata_train.csv',\n",
        "    'Attack_free_KIA_Soul_train.csv',\n",
        "    'Fuzzy_dataset_HY_Sonata_train.csv',\n",
        "    'Fuzzy_dataset_KIA_Soul_train.csv',\n",
        "    'Malfunction_1st_dataset_HY_Sonata_train.csv',\n",
        "    'Malfunction_1st_dataset_KIA_Soul_train.csv',\n",
        "    'Malfunction_2nd_HY_Sonata_train.csv',\n",
        "    'Malfunction_2nd_KIA_Soul_train.csv',\n",
        "    'Replay_dataset_HY_Sonata_train.csv',\n",
        "    'Replay_dataset_KIA_Soul_train.csv'\n",
        "]\n",
        "\n",
        "\n",
        "# נתיב לתיקייה המכילה את הקבצים\n",
        "\n",
        "# הפעלת הפונקציה על כל קובץ\n",
        "for fname in csv_files:\n",
        "    try:\n",
        "        cleaned_path = clean_and_save_csv(str(fname))\n",
        "        print(f\"[✓] Cleaned: {cleaned_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"[✗] Failed: {fname} — {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# caae_full.py –  Conv-AAE end-to-end pipeline (A→Z)\n",
        "# ============================================================\n",
        "\n",
        "import os, glob, itertools, cv2, numpy as np, pandas as pd, tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 0) GPU setup (optional)\n",
        "# ------------------------------------------------------------\n",
        "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
        "if physical_devices:\n",
        "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 1) Hyper-parameters\n",
        "# ------------------------------------------------------------\n",
        "IMG_SHAPE   = (32, 32, 2)              # <-- תמונת-קלט\n",
        "FEATURE_DIM = np.prod(IMG_SHAPE)       # 29*29*2\n",
        "N_LABELS    = 2\n",
        "\n",
        "BATCH       = 128\n",
        "EPOCHS      = 50\n",
        "\n",
        "LATENT_DIM  = 64\n",
        "λ_gp        = 10.0\n",
        "\n",
        "LR_AE = 5e-4\n",
        "LR_DZ = 1e-4\n",
        "LR_DY = 1e-4\n",
        "LR_G  = 5e-5\n",
        "\n",
        "ACTIVATION = 'elu'\n",
        "DROPOUT    = 0.2\n",
        "NORM_TYPE  = 'layer'           # layer / batch\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2) Pre-processing & TFRecord creation  (ללא שינוי לוגי)\n",
        "# ------------------------------------------------------------\n",
        "datasets = [\n",
        "    \"Attack_free_CHEVROLET_Spark_train.csv\",\n",
        "    \"Attack_free_KIA_Soul_train.csv\",\n",
        "    \"Flooding_CHEVROLET_Spark_train.csv\",\n",
        "    \"Flooding_KIA_Soul_train.csv\",\n",
        "    \"Fuzzy_CHEVROLET_Spark_train.csv\",\n",
        "    \"Attack_free_HY_Sonata_train.csv\",\n",
        "    \"Attack_free_KIA_Soul_train.csv\",\n",
        "    \"Fuzzy_dataset_HY_Sonata_train.csv\",\n",
        "    \"Fuzzy_dataset_KIA_Soul_train.csv\",\n",
        "    \"Malfunction_1st_dataset_HY_Sonata_train.csv\",\n",
        "    \"Malfunction_1st_dataset_KIA_Soul_train.csv\",\n",
        "    \"Malfunction_2nd_HY_Sonata_train.csv\",\n",
        "    \"Malfunction_2nd_KIA_Soul_train.csv\",\n",
        "    \"Replay_dataset_HY_Sonata_train.csv\",\n",
        "    \"Replay_dataset_KIA_Soul_train.csv\"\n",
        "]\n",
        "csv_map = {d: d for d in datasets}\n",
        "\n",
        "\n",
        "\n",
        "def fill_flag(row):\n",
        "    if not isinstance(row['Label'], str):\n",
        "        col = 'Data' + str(int(row['DLC']))\n",
        "        row['Flag'] = row.get(col, row['Label'])\n",
        "    return row\n",
        "\n",
        "def convert_canid_bits(cid):\n",
        "    try:\n",
        "        return np.array(list(map(int, bin(int(str(cid),16))[2:].zfill(29))), dtype=np.uint8)\n",
        "    except:\n",
        "        return np.zeros(29, dtype=np.uint8)\n",
        "\n",
        "def hex_to_int(x):\n",
        "    try: return int(str(x).strip(), 16)\n",
        "    except: return 0\n",
        "\n",
        "# Replace the preprocess_windows function with this fixed version:\n",
        "def preprocess_windows(csv_file):\n",
        "    print(f\"[DATA] Processing {csv_file}\")\n",
        "    cols = ['Timestamp','canID','DLC']+[f'Data{i}' for i in range(8)] + ['Label']\n",
        "    df   = pd.read_csv(csv_file, header=None, names=cols, low_memory=False)\n",
        "\n",
        "    df['Timestamp'] = pd.to_numeric(df['Timestamp'], errors='coerce')\n",
        "    df['DLC']       = pd.to_numeric(df['DLC'], errors='coerce').fillna(0).astype(int)\n",
        "    df = df.dropna(subset=['Timestamp','canID']).apply(fill_flag, axis=1)\n",
        "\n",
        "    for i in range(8):\n",
        "        df[f'Data{i}'] = df[f'Data{i}'].apply(hex_to_int).astype(np.uint8)\n",
        "\n",
        "    df['Label']    = df['Label'].astype(str).str.upper().eq('T').astype(np.uint8)\n",
        "    df['canBits'] = df['canID'].apply(convert_canid_bits)\n",
        "    df = df.sort_values('Timestamp')\n",
        "\n",
        "    bits_all   = np.stack(df['canBits'].values)\n",
        "    data_bytes = df[[f'Data{i}' for i in range(8)]].values\n",
        "    flags_all  = df['Label'].values\n",
        "\n",
        "    win = 29\n",
        "    N   = len(bits_all)//win\n",
        "    bits   = bits_all[:N*win].reshape(N, win, 29)\n",
        "    data   = data_bytes[:N*win].reshape(N, win, 8)\n",
        "    flags  = flags_all[:N*win].reshape(N, win)\n",
        "\n",
        "    rows = []\n",
        "    for i in range(N):\n",
        "        id_block = bits[i]\n",
        "        id_img   = cv2.resize(id_block.astype(np.uint8), (32, 32),\n",
        "                              interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "        last_b = data[i,-1,:]\n",
        "        b8     = np.unpackbits(last_b).reshape(8,8)\n",
        "        data_img = cv2.resize(b8.astype(np.float32), (32, 32),\n",
        "                              interpolation=cv2.INTER_NEAREST) > .5\n",
        "\n",
        "        two_ch = np.stack([id_img, data_img.astype(np.uint8)], axis=-1)  # 32x32x2\n",
        "        rows.append((two_ch.flatten().tolist(), int(flags[i].any())))\n",
        "    return rows\n",
        "\n",
        "\n",
        "def write_tfrecord(rows, base):\n",
        "    np.random.shuffle(rows)\n",
        "    ntr = int(.7*len(rows)); nvl = int(.15*len(rows))\n",
        "    splits = {'train':rows[:ntr], 'val':rows[ntr:ntr+nvl], 'test':rows[ntr+nvl:]}\n",
        "    for phase, chunk in splits.items():\n",
        "        with tf.io.TFRecordWriter(f\"{base}_{phase}.tfrecord\") as w:\n",
        "            for feat,lbl in chunk:\n",
        "                ex = tf.train.Example(\n",
        "                     features=tf.train.Features(feature={\n",
        "                     'features': tf.train.Feature(int64_list=tf.train.Int64List(value=feat)),\n",
        "                     'label':    tf.train.Feature(int64_list=tf.train.Int64List(value=[lbl]))}))\n",
        "                w.write(ex.SerializeToString())\n",
        "\n",
        "# צור TFRecords אם חסרים\n",
        "\n",
        "print(\"[DATA] Creating TFRecords…\")\n",
        "for d in datasets:\n",
        "    if not os.path.exists(csv_map[d]): continue\n",
        "    rows = preprocess_windows(csv_map[d])\n",
        "    normals = [r for r in rows if r[1]==0]\n",
        "    attacks = [r for r in rows if r[1]==1]\n",
        "    write_tfrecord(normals, f\"Normal_{d}\")\n",
        "    if attacks: write_tfrecord(attacks, d)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3) tf.data pipeline\n",
        "# ------------------------------------------------------------\n",
        "def parse_feat(proto):\n",
        "    fs = {'features': tf.io.FixedLenFeature([FEATURE_DIM], tf.int64),\n",
        "          'label':    tf.io.FixedLenFeature([1], tf.int64)}\n",
        "    feat = tf.io.parse_single_example(proto, fs)\n",
        "    x = tf.cast(feat['features'], tf.float32)\n",
        "    x = tf.reshape(x, IMG_SHAPE)              # <-- תמונה\n",
        "    y = tf.one_hot(tf.cast(feat['label'][0], tf.int32), N_LABELS)\n",
        "    return x, y\n",
        "\n",
        "train_files = glob.glob('Normal_*_train.tfrecord')\n",
        "train_ds = (\n",
        "    tf.data.TFRecordDataset(train_files, num_parallel_reads=tf.data.AUTOTUNE)\n",
        "    .map(parse_feat, tf.data.AUTOTUNE)\n",
        "    .map(lambda x,y: (x + tf.random.normal(tf.shape(x),0,0.01), x, y), tf.data.AUTOTUNE)\n",
        "    .shuffle(10000).repeat()\n",
        "    .batch(BATCH).prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "steps_per_epoch = sum(1 for _ in tf.data.TFRecordDataset(train_files)) // BATCH\n",
        "print(f\"[PIPE] records={steps_per_epoch*BATCH}, steps/epoch={steps_per_epoch}\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4) CAAE Model\n",
        "# ------------------------------------------------------------\n",
        "def dense_block(units):\n",
        "    layers = [tf.keras.layers.Dense(units)]\n",
        "    if NORM_TYPE=='layer': layers.append(tf.keras.layers.LayerNormalization())\n",
        "    elif NORM_TYPE=='batch': layers.append(tf.keras.layers.BatchNormalization())\n",
        "    layers.append(tf.keras.layers.Activation(ACTIVATION))\n",
        "    if DROPOUT>0: layers.append(tf.keras.layers.Dropout(DROPOUT))\n",
        "    return tf.keras.Sequential(layers)\n",
        "\n",
        "class ConvAAE(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # ---- encoder\n",
        "        self.enc_c1  = tf.keras.layers.Conv2D(32,(3,3),strides=2,padding='same',activation=ACTIVATION)\n",
        "        self.enc_c2  = tf.keras.layers.Conv2D(64,(3,3),strides=2,padding='same',activation=ACTIVATION)\n",
        "        self.enc_flat= tf.keras.layers.Flatten()\n",
        "        self.enc_fc  = dense_block(256)\n",
        "        self.z_layer = tf.keras.layers.Dense(LATENT_DIM)\n",
        "        self.y_logits= tf.keras.layers.Dense(N_LABELS)\n",
        "\n",
        "        # ---- decoder\n",
        "        self.dec_fc   = dense_block(8*8*64)\n",
        "        self.dec_reshape = tf.keras.layers.Reshape((8,8,64))\n",
        "        self.dec_t1  = tf.keras.layers.Conv2DTranspose(64,(3,3),strides=2,padding='same',activation=ACTIVATION)\n",
        "        self.dec_t2  = tf.keras.layers.Conv2DTranspose(32,(3,3),strides=2,padding='same',activation=ACTIVATION)\n",
        "        self.dec_out = tf.keras.layers.Conv2DTranspose(2,(3,3),padding='same',activation='sigmoid')\n",
        "\n",
        "        # ---- discriminators (z , y)\n",
        "        self.dz = tf.keras.Sequential([dense_block(256),\n",
        "                                       dense_block(128),\n",
        "                                       tf.keras.layers.Dense(1)])\n",
        "        self.dy = tf.keras.Sequential([dense_block(256),\n",
        "                                       dense_block(128),\n",
        "                                       tf.keras.layers.Dense(1)])\n",
        "\n",
        "    # ---------- forward passes ----------\n",
        "    def encode(self, x):\n",
        "        h = self.enc_c2(self.enc_c1(x))\n",
        "        h = self.enc_fc(self.enc_flat(h))\n",
        "        z = self.z_layer(h)\n",
        "        logits = self.y_logits(h)\n",
        "        y = tf.nn.softmax(logits)\n",
        "        return z, y, logits\n",
        "\n",
        "    def decode(self, z, y):\n",
        "        h = tf.concat([z,y], axis=1)\n",
        "        h = self.dec_fc(h)\n",
        "        h = self.dec_reshape(h)\n",
        "        h = self.dec_t2(self.dec_t1(h))\n",
        "        return self.dec_out(h)\n",
        "\n",
        "    def discriminate_z(self, z): return self.dz(z)\n",
        "    def discriminate_y(self, y): return self.dy(y)\n",
        "\n",
        "    @staticmethod\n",
        "    def gp(f, real, fake):\n",
        "        α = tf.random.uniform([real.shape[0],1],0,1)\n",
        "        inter = real + α*(fake-real)\n",
        "        with tf.GradientTape() as t:\n",
        "            t.watch(inter); p=f(inter)\n",
        "        g = t.gradient(p, inter)\n",
        "        slopes = tf.sqrt(tf.reduce_sum(tf.square(g), axis=1)+1e-8)\n",
        "        return tf.reduce_mean((slopes-1.)**2)\n",
        "\n",
        "caae = ConvAAE()\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4.5) Warm-up: לבנות את כל המשתנים לפני האופטימיזרים\n",
        "# ------------------------------------------------------------\n",
        "dummy_x = tf.zeros((1,) + IMG_SHAPE, dtype=tf.float32)\n",
        "z0, y0, _ = caae.encode(dummy_x)\n",
        "_ = caae.decode(z0, y0)\n",
        "_ = caae.discriminate_z(tf.random.normal((1, LATENT_DIM)))\n",
        "_ = caae.discriminate_y(tf.one_hot([0], depth=N_LABELS))\n",
        "print(\"[BUILD] all layer variables created:\", len(caae.trainable_variables))\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 5) Losses & optimizers  (עם רשימות קבועות)\n",
        "# ------------------------------------------------------------\n",
        "ae_vars = (\n",
        "    caae.enc_c1.trainable_variables + caae.enc_c2.trainable_variables +\n",
        "    caae.enc_fc.trainable_variables + caae.z_layer.trainable_variables +\n",
        "    caae.y_logits.trainable_variables +\n",
        "    caae.dec_fc.trainable_variables + caae.dec_reshape.trainable_variables +\n",
        "    caae.dec_t1.trainable_variables + caae.dec_t2.trainable_variables +\n",
        "    caae.dec_out.trainable_variables\n",
        ")\n",
        "dz_vars = caae.dz.trainable_variables\n",
        "dy_vars = caae.dy.trainable_variables\n",
        "enc_vars = (\n",
        "    caae.enc_c1.trainable_variables + caae.enc_c2.trainable_variables +\n",
        "    caae.enc_fc.trainable_variables + caae.z_layer.trainable_variables +\n",
        "    caae.y_logits.trainable_variables\n",
        ")\n",
        "\n",
        "mse  = tf.keras.losses.MeanSquaredError()\n",
        "ce   = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "opt_ae = tf.keras.optimizers.Adam(LR_AE)\n",
        "opt_dz = tf.keras.optimizers.Adam(LR_DZ)\n",
        "opt_dy = tf.keras.optimizers.Adam(LR_DY)\n",
        "opt_g  = tf.keras.optimizers.Adam(LR_G)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 6) Training step (tf.function)\n",
        "# ------------------------------------------------------------\n",
        "@tf.function\n",
        "def train_step(x_noisy, x_clean, y_lbl):\n",
        "    # ---------- auto-encoder ----------\n",
        "    with tf.GradientTape() as t_ae:\n",
        "        z_enc, y_enc, _ = caae.encode(x_noisy)\n",
        "        x_rec = caae.decode(z_enc, y_enc)\n",
        "        loss_re = mse(x_clean, x_rec)\n",
        "    grads = t_ae.gradient(loss_re, ae_vars)\n",
        "    opt_ae.apply_gradients(zip(grads, ae_vars))\n",
        "\n",
        "    # ---------- discriminator-z ----------\n",
        "    with tf.GradientTape() as t_dz:\n",
        "        z_real = tf.random.normal([tf.shape(x_noisy)[0], LATENT_DIM])\n",
        "        dz_r   = caae.discriminate_z(z_real)\n",
        "        dz_f   = caae.discriminate_z(z_enc)\n",
        "        gp_z   = caae.gp(caae.discriminate_z, z_real, z_enc)\n",
        "        loss_dz = tf.reduce_mean(dz_f) - tf.reduce_mean(dz_r) + λ_gp * gp_z\n",
        "    opt_dz.apply_gradients(zip(t_dz.gradient(loss_dz, dz_vars), dz_vars))\n",
        "\n",
        "    # ---------- discriminator-y ----------\n",
        "    with tf.GradientTape() as t_dy:\n",
        "        dy_r = caae.discriminate_y(y_lbl)\n",
        "        _, y_enc2, _ = caae.encode(x_clean)\n",
        "        dy_f = caae.discriminate_y(y_enc2)\n",
        "        gp_y = caae.gp(caae.discriminate_y, y_lbl, y_enc2)\n",
        "        loss_dy = tf.reduce_mean(dy_f) - tf.reduce_mean(dy_r) + λ_gp * gp_y\n",
        "    opt_dy.apply_gradients(zip(t_dy.gradient(loss_dy, dy_vars), dy_vars))\n",
        "\n",
        "    # ---------- generator / encoder adversarial ----------\n",
        "    with tf.GradientTape() as t_g:\n",
        "        z_g, y_g, logits = caae.encode(x_clean)\n",
        "        loss_g = (\n",
        "            -tf.reduce_mean(caae.discriminate_z(z_g))\n",
        "            -tf.reduce_mean(caae.discriminate_y(y_g))\n",
        "            + ce(y_lbl, logits)\n",
        "        )\n",
        "    opt_g.apply_gradients(zip(t_g.gradient(loss_g, enc_vars), enc_vars))\n",
        "\n",
        "    return loss_re, loss_dz, loss_dy, loss_g\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 7) Training loop\n",
        "# ------------------------------------------------------------\n",
        "re_hist, dz_hist, dy_hist, g_hist, val_hist = [],[],[],[],[]\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    print(f\"\\n[TRAIN] Epoch {epoch}/{EPOCHS}\")\n",
        "    ep_re=ep_dz=ep_dy=ep_g=0\n",
        "    for step,(xn, xc, y) in enumerate(train_ds.take(steps_per_epoch)):\n",
        "        lr,ldz,ldy,lg = train_step(xn, xc, y)\n",
        "        ep_re+=lr.numpy(); ep_dz+=ldz.numpy(); ep_dy+=ldy.numpy(); ep_g+=lg.numpy()\n",
        "        if step%100==0:\n",
        "            print(f\"  step {step}/{steps_per_epoch} | re={lr:.4f} dz={ldz:.4f} dy={ldy:.4f} g={lg:.4f}\")\n",
        "    re_hist.append(ep_re/steps_per_epoch)\n",
        "    dz_hist.append(ep_dz/steps_per_epoch)\n",
        "    dy_hist.append(ep_dy/steps_per_epoch)\n",
        "    g_hist .append(ep_g /steps_per_epoch)\n",
        "\n",
        "    # -------- validation recon ----------\n",
        "    val_loss, n_batches = 0,0\n",
        "    for fn in glob.glob('Normal_*_val.tfrecord'):\n",
        "        for x_val,_ in tf.data.TFRecordDataset(fn).map(parse_feat).batch(BATCH):\n",
        "            x_rec = caae.decode(*caae.encode(x_val)[0:2])\n",
        "            val_loss += mse(x_val, x_rec).numpy()\n",
        "            n_batches += 1\n",
        "    val_hist.append(val_loss/n_batches)\n",
        "    print(f\"[VAL] recon={val_hist[-1]:.4f}\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 8) Save encoder & decoder\n",
        "# ------------------------------------------------------------\n",
        "from tensorflow.keras.layers import Input, Activation, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# --- Encoder\n",
        "enc_in = Input(shape=IMG_SHAPE)\n",
        "h = caae.enc_c2(caae.enc_c1(enc_in))\n",
        "h = caae.enc_fc(caae.enc_flat(h))\n",
        "z_out = caae.z_layer(h)\n",
        "y_log = caae.y_logits(h)\n",
        "y_out = Activation('softmax')(y_log)\n",
        "encoder = Model(enc_in, [z_out,y_out], name='caae_encoder')\n",
        "\n",
        "# --- Decoder\n",
        "z_in = Input(shape=(LATENT_DIM,))\n",
        "y_in = Input(shape=(N_LABELS,))\n",
        "h2   = caae.dec_fc(Concatenate()([z_in,y_in]))\n",
        "h2   = caae.dec_reshape(h2)\n",
        "h2   = caae.dec_t2(caae.dec_t1(h2))\n",
        "dec_out = caae.dec_out(h2)\n",
        "decoder = Model([z_in,y_in], dec_out, name='caae_decoder')\n",
        "\n",
        "encoder.save('caae_encoder.keras')\n",
        "decoder.save('caae_decoder.keras')\n",
        "print(\"[SAVE] models stored\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 9) Evaluation\n",
        "# ------------------------------------------------------------\n",
        "errs, ys = [], []\n",
        "for fn in glob.glob('*_test.tfrecord'):\n",
        "    label = 0 if fn.startswith('Normal_') else 1\n",
        "    for x_batch,_ in tf.data.TFRecordDataset(fn).map(parse_feat).batch(256):\n",
        "        z_p,y_p = encoder(x_batch)\n",
        "        x_r = decoder([z_p,y_p])\n",
        "        e = tf.reduce_mean(tf.square(x_batch - x_r), axis=[1,2,3]).numpy()\n",
        "        errs.append(e); ys.append(np.full(e.shape,label))\n",
        "errs = np.concatenate(errs)\n",
        "ys   = np.concatenate(ys)\n",
        "\n",
        "fpr,tpr,ths = roc_curve(ys, errs)\n",
        "roc_auc     = auc(fpr,tpr)\n",
        "best_idx    = np.argmax(tpr-fpr)\n",
        "thr_opt     = ths[best_idx]\n",
        "\n",
        "print(f\"\\n[RESULT] ROC-AUC={roc_auc:.4f} | Thr={thr_opt:.6f} | \"\n",
        "      f\"TPR={tpr[best_idx]:.3f} | FPR={fpr[best_idx]:.3f}\")\n",
        "cm = confusion_matrix(ys, (errs>thr_opt).astype(int))\n",
        "print(\"[CM]\\n\", cm)\n",
        "print(\"[Report]\\n\", classification_report(ys,(errs>thr_opt).astype(int),\n",
        "                                          target_names=['Normal','Attack']))\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 10) Plotting\n",
        "# ------------------------------------------------------------\n",
        "# -- Reconstruction loss curves\n",
        "plt.figure(); plt.plot(re_hist,label='Train'); plt.plot(val_hist,label='Val')\n",
        "plt.xlabel('Epoch'); plt.ylabel('MSE'); plt.title('Reconstruction Loss'); plt.legend(); plt.show()\n",
        "\n",
        "# -- Adversarial losses\n",
        "plt.figure(); plt.plot(dz_hist,label='Disc-z'); plt.plot(dy_hist,label='Disc-y'); plt.plot(g_hist,label='Gen')\n",
        "plt.xlabel('Epoch'); plt.ylabel('Wasserstein'); plt.title('Adversarial Losses'); plt.legend(); plt.show()\n",
        "\n",
        "# -- ROC curve\n",
        "plt.figure(); plt.plot(fpr,tpr,label=f'AUC={roc_auc:.3f}'); plt.plot([0,1],[0,1],'k--')\n",
        "plt.xlabel('FPR'); plt.ylabel('TPR'); plt.title('ROC Curve'); plt.legend(); plt.show()\n",
        "\n",
        "# -- Confusion-matrix heat-map\n",
        "plt.figure(); plt.imshow(cm,cmap=plt.cm.Blues); plt.title('Confusion Matrix'); plt.colorbar()\n",
        "ticks = np.arange(2); classes=['Normal','Attack']\n",
        "plt.xticks(ticks,classes,rotation=45); plt.yticks(ticks,classes)\n",
        "th = cm.max()/2\n",
        "for i,j in itertools.product(range(2),range(2)):\n",
        "    plt.text(j,i,cm[i,j],ha='center',color='white' if cm[i,j]>th else 'black')\n",
        "plt.ylabel('True'); plt.xlabel('Predicted'); plt.tight_layout(); plt.show()\n",
        "\n",
        "# -- Error distributions\n",
        "plt.figure()\n",
        "plt.hist(errs[ys==0],bins=50,alpha=.5,label='Normal')\n",
        "plt.hist(errs[ys==1],bins=50,alpha=.5,label='Attack')\n",
        "plt.xlabel('Reconstruction error'); plt.ylabel('Count')\n",
        "plt.title('Error Distribution'); plt.legend(); plt.show()\n"
      ],
      "metadata": {
        "id": "J0kjZIGbBnNq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}