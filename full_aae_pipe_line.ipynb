{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSo5Z1NQBvA0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 0) GPU setup (optional)\n",
        "# ------------------------------------------------------------\n",
        "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
        "if physical_devices:\n",
        "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Hyperparameters\n",
        "# ------------------------------------------------------------\n",
        "FEATURE_DIM = 29 * 29 * 2\n",
        "N_LABELS   = 2\n",
        "BATCH      = 128\n",
        "EPOCHS     = 30\n",
        "\n",
        "# AAE-specific\n",
        "N_L1       = 1024\n",
        "N_L2       = 768\n",
        "LATENT_DIM = 64\n",
        "Î»_gp       = 10.0\n",
        "\n",
        "# Learning rates\n",
        "LR_AE = 0.0005\n",
        "LR_DZ = 0.0001\n",
        "LR_DY = 0.0001\n",
        "LR_G  = 5e-5\n",
        "\n",
        "# Architecture options\n",
        "ACTIVATION = 'elu'\n",
        "DROPOUT    = 0.2\n",
        "NORM_TYPE  = 'layer'  # 'layer' or 'batch'\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 1) Helper functions: preprocessing & TFRecord creation\n",
        "# ------------------------------------------------------------\n",
        "datasets = [\n",
        "    \"Attack_free_CHEVROLET_Spark_train.csv\",\n",
        "    \"Attack_free_KIA_Soul_train.csv\",\n",
        "    \"Flooding_CHEVROLET_Spark_train.csv\",\n",
        "    \"Flooding_KIA_Soul_train.csv\",\n",
        "    \"Fuzzy_CHEVROLET_Spark_train.csv\",\n",
        "    \"Attack_free_HY_Sonata_train.csv\",\n",
        "    \"Attack_free_KIA_Soul_train.csv\",\n",
        "    \"Fuzzy_dataset_HY_Sonata_train.csv\",\n",
        "    \"Fuzzy_dataset_KIA_Soul_train.csv\",\n",
        "    \"Malfunction_1st_dataset_HY_Sonata_train.csv\",\n",
        "    \"Malfunction_1st_dataset_KIA_Soul_train.csv\",\n",
        "    \"Malfunction_2nd_HY_Sonata_train.csv\",\n",
        "    \"Malfunction_2nd_KIA_Soul_train.csv\",\n",
        "    \"Replay_dataset_HY_Sonata_train.csv\",\n",
        "    \"Replay_dataset_KIA_Soul_train.csv\"\n",
        "]\n",
        "csv_map = {d: d for d in datasets}\n",
        "\n",
        "def fill_flag(row):\n",
        "    if not isinstance(row['Flag'], str):\n",
        "        col = 'Data' + str(int(row['DLC']))\n",
        "        row['Flag'] = row.get(col, row['Flag'])\n",
        "    return row\n",
        "\n",
        "def convert_canid_bits(cid):\n",
        "    try:\n",
        "        b = bin(int(str(cid), 16))[2:].zfill(29)\n",
        "        return np.array(list(map(int, b)), dtype=np.uint8)\n",
        "    except:\n",
        "        return np.zeros(29, dtype=np.uint8)\n",
        "\n",
        "def hex_to_int(x):\n",
        "    try:\n",
        "        return int(str(x).strip(), 16)\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "def preprocess_windows(csv_file):\n",
        "    print(f\"[DATA] Processing {csv_file}\")\n",
        "    attrs = ['Timestamp', 'canID', 'DLC'] + [f'Data{i}' for i in range(8)] + ['Flag']\n",
        "    df = pd.read_csv(csv_file, header=None, names=attrs, low_memory=False)\n",
        "    df['Timestamp'] = pd.to_numeric(df['Timestamp'], errors='coerce')\n",
        "    df['DLC']       = pd.to_numeric(df['DLC'], errors='coerce').fillna(0).astype(int)\n",
        "    df = df.dropna(subset=['Timestamp', 'canID']).apply(fill_flag, axis=1)\n",
        "    for i in range(8):\n",
        "        df[f'Data{i}'] = df[f'Data{i}'].apply(hex_to_int).astype(np.uint8)\n",
        "    df['Flag']    = df['Flag'].astype(str).str.upper().eq('T').astype(np.uint8)\n",
        "    df['canBits'] = df['canID'].apply(convert_canid_bits)\n",
        "    df = df.sort_values('Timestamp')\n",
        "\n",
        "    bits_all   = np.stack(df['canBits'].values)\n",
        "    data_bytes = df[[f'Data{i}' for i in range(8)]].values\n",
        "    flags_all  = df['Flag'].values\n",
        "\n",
        "    win = 29\n",
        "    N   = len(bits_all) // win\n",
        "    bits   = bits_all[:N * win].reshape(N, win, 29)\n",
        "    data   = data_bytes[:N * win].reshape(N, win, 8)\n",
        "    flags  = flags_all[:N * win].reshape(N, win)\n",
        "\n",
        "    rows = []\n",
        "    for i in range(N):\n",
        "        id_img   = bits[i].astype(np.uint8)\n",
        "        last_b   = data[i, -1, :]\n",
        "        b8       = np.unpackbits(last_b, axis=0).reshape(8,8)\n",
        "        data_img = cv2.resize(b8.astype(np.float32), (29,29), interpolation=cv2.INTER_NEAREST) > 0.5\n",
        "        two_ch   = np.stack([id_img, data_img.astype(np.uint8)], axis=-1)\n",
        "        feat_int = two_ch.flatten().tolist()\n",
        "        lbl      = int(flags[i].any())\n",
        "        rows.append((feat_int, lbl))\n",
        "    return rows\n",
        "\n",
        "def write_tfrecord(rows, base):\n",
        "    np.random.shuffle(rows)\n",
        "    ntr = int(0.7 * len(rows))\n",
        "    nvl = int(0.15 * len(rows))\n",
        "    splits = {'train': rows[:ntr], 'val': rows[ntr:ntr+nvl], 'test': rows[ntr+ntr+nvl:]} if False else {'train': rows[:ntr], 'val': rows[ntr:ntr+nvl], 'test': rows[ntr+nvl:]}\n",
        "    for ph, ch in splits.items():\n",
        "        fn = f\"{base}_{ph}.tfrecord\"\n",
        "        with tf.io.TFRecordWriter(fn) as writer:\n",
        "            for feat, lbl in ch:\n",
        "                ex = tf.train.Example(features=tf.train.Features(feature={\n",
        "                    'features': tf.train.Feature(int64_list=tf.train.Int64List(value=feat)),\n",
        "                    'label':    tf.train.Feature(int64_list=tf.train.Int64List(value=[lbl]))\n",
        "                }))\n",
        "                writer.write(ex.SerializeToString())\n",
        "\n",
        "# Create/check TFRecords\n",
        "expected = []\n",
        "for a in datasets:\n",
        "    for ph in ('train', 'val', 'test'):\n",
        "        expected.append(f\"{a}_{ph}.tfrecord\")\n",
        "        if a != 'parsed_dataset':\n",
        "            expected.append(f\"Normal_{a}_{ph}.tfrecord\")\n",
        "if not all(os.path.exists(f) for f in expected):\n",
        "    print(\"[DATA] TFRecords missing, preprocessing...\")\n",
        "    for a in datasets:\n",
        "        src = csv_map[a]\n",
        "        if not os.path.exists(src):\n",
        "            print(f\"[WARN] {src} not found\")\n",
        "        else:\n",
        "            rows    = preprocess_windows(src)\n",
        "            normals = [r for r in rows if r[1] == 0]\n",
        "            attacks = [r for r in rows if r[1] == 1]\n",
        "            write_tfrecord(normals, f\"Normal_{a}\")\n",
        "            if attacks:\n",
        "                write_tfrecord(attacks, a)\n",
        "else:\n",
        "    print(\"[DATA] All TFRecords found.\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2) tf.data pipeline\n",
        "# ------------------------------------------------------------\n",
        "def parse_feat(proto):\n",
        "    feat = tf.io.parse_single_example(proto, {\n",
        "        'features': tf.io.FixedLenFeature([FEATURE_DIM], tf.int64),\n",
        "        'label':    tf.io.FixedLenFeature([1], tf.int64)\n",
        "    })\n",
        "    x = tf.cast(feat['features'], tf.float32)\n",
        "    y = tf.one_hot(tf.cast(feat['label'][0], tf.int32), N_LABELS)\n",
        "    return x, y\n",
        "\n",
        "train_files = glob.glob('Normal_*_train.tfrecord')\n",
        "train_ds = (\n",
        "    tf.data.TFRecordDataset(train_files, num_parallel_reads=tf.data.AUTOTUNE)\n",
        "    .map(parse_feat, tf.data.AUTOTUNE)\n",
        "    .map(lambda x, y: (x + tf.random.normal(tf.shape(x), 0, 0.01), x, y), tf.data.AUTOTUNE)\n",
        "    .shuffle(10000).repeat()\n",
        "    .batch(BATCH).prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "total = sum(1 for _ in tf.data.TFRecordDataset(train_files))\n",
        "steps_per_epoch = total // BATCH\n",
        "print(f\"[PIPE] Total records: {total}, steps/epoch: {steps_per_epoch}\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3) AAE Model definition\n",
        "# ------------------------------------------------------------\n",
        "class AAE(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        def dense_block(units):\n",
        "            layers = [tf.keras.layers.Dense(units)]\n",
        "            if NORM_TYPE == 'layer': layers.append(tf.keras.layers.LayerNormalization())\n",
        "            elif NORM_TYPE == 'batch': layers.append(tf.keras.layers.BatchNormalization())\n",
        "            layers.append(tf.keras.layers.Activation(ACTIVATION))\n",
        "            if DROPOUT > 0: layers.append(tf.keras.layers.Dropout(DROPOUT))\n",
        "            return tf.keras.Sequential(layers)\n",
        "\n",
        "        self.e1   = dense_block(N_L1)\n",
        "        self.e2   = dense_block(N_L2)\n",
        "        self.ez   = tf.keras.layers.Dense(LATENT_DIM)\n",
        "        self.ey   = tf.keras.layers.Dense(N_LABELS)\n",
        "\n",
        "        self.d1   = dense_block(N_L2)\n",
        "        self.d2   = dense_block(N_L1)\n",
        "        self.dout = tf.keras.layers.Dense(FEATURE_DIM, activation='sigmoid')\n",
        "\n",
        "        self.dz1  = dense_block(N_L1)\n",
        "        self.dz2  = dense_block(N_L2)\n",
        "        self.dzout= tf.keras.layers.Dense(1)\n",
        "\n",
        "        self.dy1  = dense_block(N_L1)\n",
        "        self.dy2  = dense_block(N_L2)\n",
        "        self.dyout= tf.keras.layers.Dense(1)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h      = self.e2(self.e1(x))\n",
        "        z      = self.ez(h)\n",
        "        logits = self.ey(h)\n",
        "        return z, tf.nn.softmax(logits), logits\n",
        "\n",
        "    def decode(self, z, y):\n",
        "        h = tf.concat([z, y], axis=1)\n",
        "        h = self.d1(h)\n",
        "        h = self.d2(h)\n",
        "        return self.dout(h)\n",
        "\n",
        "    def discriminate_z(self, z):\n",
        "        h = self.dz1(z)\n",
        "        h = self.dz2(h)\n",
        "        return self.dzout(h)\n",
        "\n",
        "    def discriminate_y(self, y):\n",
        "        h = self.dy1(y)\n",
        "        h = self.dy2(h)\n",
        "        return self.dyout(h)\n",
        "\n",
        "    def gradient_penalty(self, f, real, fake):\n",
        "        alpha = tf.random.uniform([real.shape[0], 1], 0, 1)\n",
        "        interm = real + alpha * (fake - real)\n",
        "        with tf.GradientTape() as tape:\n",
        "            tape.watch(interm)\n",
        "            pred = f(interm)\n",
        "        grads = tape.gradient(pred, interm)\n",
        "        slopes= tf.sqrt(tf.reduce_sum(tf.square(grads), axis=1) + 1e-8)\n",
        "        return tf.reduce_mean((slopes - 1)**2)\n",
        "\n",
        "aae = AAE()\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4) Losses & Optimizers\n",
        "# ------------------------------------------------------------\n",
        "mse    = tf.keras.losses.MeanSquaredError()\n",
        "ce     = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "opt_ae = tf.keras.optimizers.Adam(LR_AE)\n",
        "opt_dz = tf.keras.optimizers.Adam(LR_DZ)\n",
        "opt_dy = tf.keras.optimizers.Adam(LR_DY)\n",
        "opt_g  = tf.keras.optimizers.Adam(LR_G)\n",
        "\n",
        "# Lists to track losses\n",
        "train_re_losses = []\n",
        "val_re_losses   = []\n",
        "train_dz_losses = []\n",
        "train_dy_losses = []\n",
        "train_g_losses  = []\n",
        "\n",
        "@tf.function\n",
        "def train_step(xn, xc, y):\n",
        "    with tf.GradientTape() as t_ae:\n",
        "        z, yp, logits = aae.encode(xn)\n",
        "        xr = aae.decode(z, yp)\n",
        "        loss_re = mse(xc, xr)\n",
        "    vars_ae = aae.e1.trainable_variables + aae.e2.trainable_variables + aae.ez.trainable_variables + aae.ey.trainable_variables + aae.d1.trainable_variables + aae.d2.trainable_variables + aae.dout.trainable_variables\n",
        "    grads_ae = t_ae.gradient(loss_re, vars_ae)\n",
        "    opt_ae.apply_gradients(zip(grads_ae, vars_ae))\n",
        "\n",
        "    with tf.GradientTape() as t_dz:\n",
        "        z_real = tf.random.normal([xn.shape[0], LATENT_DIM])\n",
        "        dz_r = aae.discriminate_z(z_real)\n",
        "        dz_f = aae.discriminate_z(z)\n",
        "        gp   = aae.gradient_penalty(aae.discriminate_z, z_real, z)\n",
        "        loss_dz = tf.reduce_mean(dz_f) - tf.reduce_mean(dz_r) + Î»_gp * gp\n",
        "    vars_dz = aae.dz1.trainable_variables + aae.dz2.trainable_variables + aae.dzout.trainable_variables\n",
        "    grads_dz = t_dz.gradient(loss_dz, vars_dz)\n",
        "    opt_dz.apply_gradients(zip(grads_dz, vars_dz))\n",
        "\n",
        "    with tf.GradientTape() as t_dy:\n",
        "        dy_r = aae.discriminate_y(y)\n",
        "        _, yp_enc, _ = aae.encode(xc)\n",
        "        dy_f = aae.discriminate_y(yp_enc)\n",
        "        gp_y = aae.gradient_penalty(aae.discriminate_y, y, yp_enc)\n",
        "        loss_dy = tf.reduce_mean(dy_f) - tf.reduce_mean(dy_r) + Î»_gp * gp_y\n",
        "    vars_dy = aae.dy1.trainable_variables + aae.dy2.trainable_variables + aae.dyout.trainable_variables\n",
        "    grads_dy = t_dy.gradient(loss_dy, vars_dy)\n",
        "    opt_dy.apply_gradients(zip(grads_dy, vars_dy))\n",
        "\n",
        "    with tf.GradientTape() as t_g:\n",
        "        z_enc, y_enc, logits_enc = aae.encode(xc)\n",
        "        loss_g = -tf.reduce_mean(aae.discriminate_z(z_enc))\n",
        "        loss_g += -tf.reduce_mean(aae.discriminate_y(y_enc))\n",
        "        loss_g += ce(y, logits_enc)\n",
        "    vars_g = aae.e1.trainable_variables + aae.e2.trainable_variables + aae.ez.trainable_variables + aae.ey.trainable_variables\n",
        "    grads_g = t_g.gradient(loss_g, vars_g)\n",
        "    opt_g.apply_gradients(zip(grads_g, vars_g))\n",
        "\n",
        "    return loss_re, loss_dz, loss_dy, loss_g\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 5) Training loop\n",
        "# ------------------------------------------------------------\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    print(f\"[TRAIN] Epoch {epoch}/{EPOCHS}\")\n",
        "    epoch_re, epoch_dz, epoch_dy, epoch_g = 0, 0, 0, 0\n",
        "    it = iter(train_ds)\n",
        "    for step in range(steps_per_epoch):\n",
        "        xn, xc, y = next(it)\n",
        "        lr, ldz, ldy, lg = train_step(xn, xc, y)\n",
        "        epoch_re  += lr.numpy()\n",
        "        epoch_dz += ldz.numpy()\n",
        "        epoch_dy += ldy.numpy()\n",
        "        epoch_g  += lg.numpy()\n",
        "        if step % 100 == 0:\n",
        "            print(f\" step {step}/{steps_per_epoch} | recon={lr:.4f} dz={ldz:.4f} dy={ldy:.4f} gen={lg:.4f}\")\n",
        "\n",
        "    # average losses\n",
        "    train_re_losses.append(epoch_re/steps_per_epoch)\n",
        "    train_dz_losses.append(epoch_dz/steps_per_epoch)\n",
        "    train_dy_losses.append(epoch_dy/steps_per_epoch)\n",
        "    train_g_losses.append(epoch_g/steps_per_epoch)\n",
        "\n",
        "    # validation recon loss\n",
        "    val_loss, val_steps = 0, 0\n",
        "    val_files = glob.glob('Normal_*_val.tfrecord')\n",
        "    for fn in val_files:\n",
        "        ds_val = tf.data.TFRecordDataset(fn).map(parse_feat).batch(BATCH)\n",
        "        for x_val, _ in ds_val:\n",
        "            _, yp, _ = aae.encode(x_val + tf.random.normal(tf.shape(x_val),0,0.01))\n",
        "            x_rec = aae.decode(*aae.encode(x_val)[0:2])\n",
        "            val_loss += tf.reduce_mean(mse(x_val, x_rec)).numpy()\n",
        "            val_steps += 1\n",
        "    val_re_losses.append(val_loss/val_steps)\n",
        "    print(f\"[VALID] recon={val_re_losses[-1]:.4f}\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 6) Save encoder & decoder\n",
        "# ------------------------------------------------------------\n",
        "from tensorflow.keras.layers import Input, Activation, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "enc_in = Input(shape=(FEATURE_DIM,))\n",
        "h = aae.e2(aae.e1(enc_in))\n",
        "z_enc = aae.ez(h)\n",
        "y_logits = aae.ey(h)\n",
        "y_enc = Activation('softmax')(y_logits)\n",
        "encoder = Model(enc_in, [z_enc, y_enc], name='aae_encoder')\n",
        "\n",
        "z_in = Input(shape=(LATENT_DIM,))\n",
        "y_in = Input(shape=(N_LABELS,))\n",
        "h2 = aae.d2(aae.d1(Concatenate()([z_in, y_in])))\n",
        "dec_out = aae.dout(h2)\n",
        "decoder = Model([z_in, y_in], dec_out, name='aae_decoder')\n",
        "\n",
        "encoder.save('aae_encoder.keras')\n",
        "decoder.save('aae_decoder.keras')\n",
        "print(\"[SAVE] Encoder & decoder saved\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 7) Evaluation\n",
        "# ------------------------------------------------------------\n",
        "errs, ys = [], []\n",
        "for fn in glob.glob('*_test.tfrecord'):\n",
        "    label = 0 if fn.startswith('Normal_') else 1\n",
        "    ds_eval = tf.data.TFRecordDataset(fn).map(parse_feat).batch(256)\n",
        "    for x_batch, _ in ds_eval:\n",
        "        z_p, y_p = encoder(x_batch)\n",
        "        x_r = decoder([z_p, y_p])\n",
        "        e = tf.reduce_mean((x_batch - x_r)**2, axis=1).numpy()\n",
        "        errs.append(e)\n",
        "        ys.append(np.full(e.shape, label))\n",
        "errs = np.concatenate(errs)\n",
        "ys   = np.concatenate(ys)\n",
        "\n",
        "fpr, tpr, ths = roc_curve(ys, errs)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "opt_idx = np.argmax(tpr - fpr)\n",
        "opt_thr = ths[opt_idx]\n",
        "\n",
        "print(f\"[RESULT] ROC AUC: {roc_auc:.4f}, Thr: {opt_thr:.6f}, TPR: {tpr[opt_idx]:.3f}, FPR: {fpr[opt_idx]:.3f}\")\n",
        "print(\"[RESULT] Confusion Matrix:\")\n",
        "cm = confusion_matrix(ys, (errs > opt_thr).astype(int))\n",
        "print(cm)\n",
        "print(\"[RESULT] Classification Report:\")\n",
        "print(classification_report(ys, (errs > opt_thr).astype(int), target_names=['Normal','Attack']))\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 8) Plotting\n",
        "# ------------------------------------------------------------\n",
        "# Reconstruction loss curves\n",
        "plt.figure()\n",
        "plt.plot(range(1, EPOCHS+1), train_re_losses, label='Train recon')\n",
        "plt.plot(range(1, EPOCHS+1), val_re_losses,   label='Val recon')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MSE Loss')\n",
        "plt.title('Reconstruction Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Adversarial losses\n",
        "plt.figure()\n",
        "plt.plot(range(1, EPOCHS+1), train_dz_losses, label='Disc_z')\n",
        "plt.plot(range(1, EPOCHS+1), train_dy_losses, label='Disc_y')\n",
        "plt.plot(range(1, EPOCHS+1), train_g_losses,  label='Generator')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Wasserstein Loss')\n",
        "plt.title('Adversarial Losses')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# ROC curve\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.3f}')\n",
        "plt.plot([0,1],[0,1],'k--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve (Test Set)')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Confusion matrix heatmap\n",
        "plt.figure()\n",
        "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.colorbar()\n",
        "classes = ['Normal','Attack']\n",
        "tick_marks = np.arange(len(classes))\n",
        "plt.xticks(tick_marks, classes, rotation=45)\n",
        "plt.yticks(tick_marks, classes)\n",
        "thresh = cm.max() / 2\n",
        "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "    plt.text(j, i, cm[i, j], horizontalalignment=\"center\",\n",
        "             color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Error distribution histogram\n",
        "plt.figure()\n",
        "plt.hist(errs[ys==0], bins=50, alpha=0.5, label='Normal')\n",
        "plt.hist(errs[ys==1], bins=50, alpha=0.5, label='Attack')\n",
        "plt.xlabel('Reconstruction error')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Error Distribution')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    }
  ]
}