{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "last version cae 26.04.25"
      ],
      "metadata": {
        "id": "lIpykt6p1c5Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZyZ0xKV1bvI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6006ab04-41dd-4480-948a-0293f4236e94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”„ Rebuilding TFRecords from CSV...\n",
            "\n",
            "âŽ¯âŽ¯âŽ¯âŽ¯ DoS âŽ¯âŽ¯âŽ¯âŽ¯\n",
            "DoS_dataset.csv: #Normal=62208, #Attack=26276\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DoS: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26276/26276 [00:05<00:00, 4923.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  â†’ DoS_train.tfrecord: 18393 records\n",
            "  â†’ DoS_val.tfrecord: 3941 records\n",
            "  â†’ DoS_test.tfrecord: 3942 records\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Normal_DoS: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62208/62208 [00:12<00:00, 4887.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  â†’ Normal_DoS_train.tfrecord: 43545 records\n",
            "  â†’ Normal_DoS_val.tfrecord: 9331 records\n",
            "  â†’ Normal_DoS_test.tfrecord: 9332 records\n",
            "\n",
            "âŽ¯âŽ¯âŽ¯âŽ¯ Fuzzy âŽ¯âŽ¯âŽ¯âŽ¯\n",
            "Fuzzy_dataset.csv: #Normal=61478, #Attack=31184\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fuzzy: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31184/31184 [00:06<00:00, 4548.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  â†’ Fuzzy_train.tfrecord: 21828 records\n",
            "  â†’ Fuzzy_val.tfrecord: 4677 records\n",
            "  â†’ Fuzzy_test.tfrecord: 4679 records\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Normal_Fuzzy: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61478/61478 [00:12<00:00, 4896.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  â†’ Normal_Fuzzy_train.tfrecord: 43034 records\n",
            "  â†’ Normal_Fuzzy_val.tfrecord: 9221 records\n",
            "  â†’ Normal_Fuzzy_test.tfrecord: 9223 records\n",
            "\n",
            "âŽ¯âŽ¯âŽ¯âŽ¯ RPM âŽ¯âŽ¯âŽ¯âŽ¯\n",
            "RPM_dataset.csv: #Normal=61531, #Attack=50027\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "RPM: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50027/50027 [00:10<00:00, 4689.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  â†’ RPM_train.tfrecord: 35018 records\n",
            "  â†’ RPM_val.tfrecord: 7504 records\n",
            "  â†’ RPM_test.tfrecord: 7505 records\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Normal_RPM: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61531/61531 [00:12<00:00, 4865.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  â†’ Normal_RPM_train.tfrecord: 43071 records\n",
            "  â†’ Normal_RPM_val.tfrecord: 9229 records\n",
            "  â†’ Normal_RPM_test.tfrecord: 9231 records\n",
            "\n",
            "âŽ¯âŽ¯âŽ¯âŽ¯ gear âŽ¯âŽ¯âŽ¯âŽ¯\n",
            "gear_dataset.csv: #Normal=61462, #Attack=45763\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "gear: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45763/45763 [00:09<00:00, 4637.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  â†’ gear_train.tfrecord: 32034 records\n",
            "  â†’ gear_val.tfrecord: 6864 records\n",
            "  â†’ gear_test.tfrecord: 6865 records\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Normal_gear: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61462/61462 [00:12<00:00, 4867.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  â†’ Normal_gear_train.tfrecord: 43023 records\n",
            "  â†’ Normal_gear_val.tfrecord: 9219 records\n",
            "  â†’ Normal_gear_test.tfrecord: 9220 records\n",
            "\n",
            "âŽ¯âŽ¯âŽ¯âŽ¯ parsed_dataset âŽ¯âŽ¯âŽ¯âŽ¯\n",
            "parsed_dataset.csv: #Normal=34098, #Attack=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Normal_parsed_dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34098/34098 [00:07<00:00, 4591.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  â†’ Normal_parsed_dataset_train.tfrecord: 23868 records\n",
            "  â†’ Normal_parsed_dataset_val.tfrecord: 5114 records\n",
            "  â†’ Normal_parsed_dataset_test.tfrecord: 5116 records\n",
            "\n",
            "âœ… train_ds files: 5, val_ds files: 5\n",
            "Epoch 1/25\n",
            "   3071/Unknown \u001b[1m20s\u001b[0m 4ms/step - loss: 0.0363"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m3071/3071\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 5ms/step - loss: 0.0363 - val_loss: 0.0222\n",
            "Epoch 2/25\n",
            "\u001b[1m3071/3071\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - loss: 0.0179 - val_loss: 0.0184\n",
            "Epoch 3/25\n",
            "\u001b[1m3071/3071\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - loss: 0.0149 - val_loss: 0.0159\n",
            "Epoch 4/25\n",
            "\u001b[1m3071/3071\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - loss: 0.0128 - val_loss: 0.0148\n",
            "Epoch 5/25\n",
            "\u001b[1m3071/3071\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - loss: 0.0113 - val_loss: 0.0124\n",
            "Epoch 6/25\n",
            "\u001b[1m3071/3071\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - loss: 0.0101 - val_loss: 0.0116\n",
            "Epoch 7/25\n",
            "\u001b[1m3071/3071\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - loss: 0.0093 - val_loss: 0.0112\n",
            "Epoch 8/25\n",
            "\u001b[1m3071/3071\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - loss: 0.0087 - val_loss: 0.0104\n",
            "Epoch 9/25\n",
            "\u001b[1m3071/3071\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - loss: 0.0082 - val_loss: 0.0107\n",
            "Epoch 10/25\n",
            "\u001b[1m3071/3071\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - loss: 0.0078 - val_loss: 0.0091\n",
            "Epoch 11/25\n",
            "\u001b[1m3071/3071\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - loss: 0.0075 - val_loss: 0.0091\n",
            "Epoch 12/25\n",
            "\u001b[1m3071/3071\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - loss: 0.0072 - val_loss: 0.0085\n",
            "Epoch 13/25\n",
            "\u001b[1m3071/3071\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - loss: 0.0070 - val_loss: 0.0085\n",
            "Epoch 14/25\n",
            "\u001b[1m3071/3071\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - loss: 0.0067 - val_loss: 0.0078\n",
            "Epoch 15/25\n",
            "\u001b[1m3071/3071\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - loss: 0.0065 - val_loss: 0.0076\n",
            "Epoch 16/25\n",
            "\u001b[1m3071/3071\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - loss: 0.0063 - val_loss: 0.0077\n",
            "Epoch 17/25\n",
            "\u001b[1m3071/3071\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - loss: 0.0061 - val_loss: 0.0072\n",
            "Epoch 18/25\n",
            "\u001b[1m3071/3071\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - loss: 0.0060 - val_loss: 0.0072\n",
            "Epoch 19/25\n",
            "\u001b[1m3071/3071\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - loss: 0.0059 - val_loss: 0.0070\n",
            "Epoch 20/25\n",
            "\u001b[1m3071/3071\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - loss: 0.0057 - val_loss: 0.0070\n",
            "Epoch 21/25\n",
            "\u001b[1m3071/3071\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - loss: 0.0056 - val_loss: 0.0069\n",
            "Epoch 22/25\n",
            "\u001b[1m3071/3071\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - loss: 0.0055 - val_loss: 0.0065\n",
            "Epoch 23/25\n",
            "\u001b[1m3071/3071\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - loss: 0.0054 - val_loss: 0.0064\n",
            "Epoch 24/25\n",
            "\u001b[1m3071/3071\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - loss: 0.0053 - val_loss: 0.0065\n",
            "Epoch 25/25\n",
            "\u001b[1m3071/3071\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - loss: 0.0052 - val_loss: 0.0064\n",
            "\n",
            "ROC AUC = 0.9733\n",
            "Youdenâ€™s J optimal thresh = 0.009591  (TPR=0.913, FPR=0.068)\n",
            "\n",
            "Confusion Matrix @ optimal thresh:\n",
            " [[39245  2869]\n",
            " [ 2007 20984]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "      Normal       0.95      0.93      0.94     42114\n",
            "      Attack       0.88      0.91      0.90     22991\n",
            "\n",
            "    accuracy                           0.93     65105\n",
            "   macro avg       0.92      0.92      0.92     65105\n",
            "weighted avg       0.93      0.93      0.93     65105\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report\n",
        "from tensorflow.keras.saving import register_keras_serializable\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 0) GPU setup (optional)\n",
        "# ------------------------------------------------------------\n",
        "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
        "if physical_devices:\n",
        "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 1) Define datasets & CSVâ†’TFRecord mapping\n",
        "# ------------------------------------------------------------\n",
        "datasets = ['DoS','Fuzzy','RPM','gear','parsed_dataset']\n",
        "csv_map   = {\n",
        "    'DoS':            'DoS_dataset.csv',\n",
        "    'Fuzzy':          'Fuzzy_dataset.csv',\n",
        "    'RPM':            'RPM_dataset.csv',\n",
        "    'gear':           'gear_dataset.csv',\n",
        "    'parsed_dataset': 'parsed_dataset.csv'\n",
        "}\n",
        "\n",
        "# build the full list of expected TFRecord files\n",
        "expected = []\n",
        "for a in datasets:\n",
        "    for phase in ('train','val','test'):\n",
        "        expected.append(f\"{a}_{phase}.tfrecord\")\n",
        "        if a != 'parsed_dataset':\n",
        "            expected.append(f\"Normal_{a}_{phase}.tfrecord\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2) Preâ€flight: if any missing â†’ rebuild them, else skip\n",
        "# ------------------------------------------------------------\n",
        "if all(os.path.exists(fn) for fn in expected):\n",
        "    print(\"âœ… All TFRecords found, skipping CSVâ†’TFRecord.\")\n",
        "else:\n",
        "    # remove any leftovers\n",
        "    for fn in glob.glob(\"*_train.tfrecord\") + glob.glob(\"*_val.tfrecord\") + glob.glob(\"*_test.tfrecord\"):\n",
        "        os.remove(fn)\n",
        "    print(\"ðŸ”„ Rebuilding TFRecords from CSV...\")\n",
        "\n",
        "    # helper fns\n",
        "    attributes = ['Timestamp','canID','DLC'] + [f'Data{i}' for i in range(8)] + ['Flag']\n",
        "    def fill_flag(row):\n",
        "        if not isinstance(row['Flag'], str):\n",
        "            col = 'Data'+str(int(row['DLC']))\n",
        "            row['Flag'] = row.get(col, row['Flag'])\n",
        "        return row\n",
        "    def convert_canid_bits(cid):\n",
        "        try:\n",
        "            b = bin(int(str(cid),16))[2:].zfill(29)\n",
        "            return np.array(list(map(int,b)),dtype=np.uint8)\n",
        "        except:\n",
        "            return np.zeros(29,dtype=np.uint8)\n",
        "    def hex_to_int(x):\n",
        "        try:    return int(str(x).strip(),16)\n",
        "        except:\n",
        "            try: return int(x)\n",
        "            except: return 0\n",
        "\n",
        "    def preprocess_windows(csv_file):\n",
        "        df = pd.read_csv(csv_file, header=None, names=attributes, low_memory=False)\n",
        "        df['Timestamp'] = pd.to_numeric(df['Timestamp'], errors='coerce')\n",
        "        df['DLC']       = pd.to_numeric(df['DLC'], errors='coerce').fillna(0).astype(int)\n",
        "        df = df.dropna(subset=['Timestamp','canID']).apply(fill_flag, axis=1)\n",
        "        for i in range(8):\n",
        "            df[f'Data{i}'] = df[f'Data{i}'].apply(hex_to_int).astype(np.uint8)\n",
        "        df['Flag'] = df['Flag'].astype(str).str.upper().eq('T').astype(np.uint8)\n",
        "        df['canBits'] = df['canID'].apply(convert_canid_bits)\n",
        "        df = df.sort_values('Timestamp')\n",
        "\n",
        "        bits_all   = np.stack(df['canBits'].values)\n",
        "        flags_all  = df['Flag'].values\n",
        "        data_bytes = df[[f'Data{i}' for i in range(8)]].values\n",
        "\n",
        "        win   = 29\n",
        "        num_w = bits_all.shape[0] // win\n",
        "\n",
        "        bits_trunc  = bits_all[:num_w*win].reshape(num_w,win,29)\n",
        "        flags_trunc = flags_all[:num_w*win].reshape(num_w,win)\n",
        "        data_trunc  = data_bytes[:num_w*win].reshape(num_w,win,8)\n",
        "\n",
        "        last_bytes = data_trunc[:,-1,:]\n",
        "        bits64     = np.unpackbits(last_bytes,axis=1).reshape(-1,8,8)\n",
        "        data_imgs  = [cv2.resize(b.astype(np.float32),(29,29),\n",
        "                                  interpolation=cv2.INTER_NEAREST)\n",
        "                      for b in bits64]\n",
        "\n",
        "        rows=[]\n",
        "        for i in range(num_w):\n",
        "            id_img   = bits_trunc[i].astype(np.uint8)\n",
        "            data_img = (data_imgs[i]>0.5).astype(np.uint8)\n",
        "            two_ch   = np.stack([id_img,data_img],axis=-1)\n",
        "            feat_int = two_ch.flatten().astype(np.int64).tolist()\n",
        "            lbl      = int(flags_trunc[i].any())\n",
        "            rows.append({'features':feat_int,'label':lbl})\n",
        "\n",
        "        out = pd.DataFrame(rows)\n",
        "        print(f\"{csv_file}: #Normal={(out.label==0).sum()}, #Attack={(out.label==1).sum()}\")\n",
        "        return out\n",
        "\n",
        "    def serialize_example(x,y):\n",
        "        return tf.train.Example(features=tf.train.Features(feature={\n",
        "            'input_features': tf.train.Feature(int64_list=tf.train.Int64List(value=x)),\n",
        "            'label':          tf.train.Feature(int64_list=tf.train.Int64List(value=[y]))\n",
        "        })).SerializeToString()\n",
        "\n",
        "    def split_and_write(df, base):\n",
        "        tfname = f\"{base}.tfrecord\"\n",
        "        with tf.io.TFRecordWriter(tfname) as w:\n",
        "            for _,r in tqdm(df.iterrows(), total=len(df), desc=base):\n",
        "                w.write(serialize_example(r.features,r.label))\n",
        "        recs = list(tf.data.TFRecordDataset([tfname]).as_numpy_iterator())\n",
        "        np.random.shuffle(recs)\n",
        "        n   = len(recs)\n",
        "        ntr = int(0.7*n); nvl = int(0.15*n)\n",
        "        for phase, chunk in zip(['train','val','test'],\n",
        "                                [recs[:ntr], recs[ntr:ntr+nvl], recs[ntr+nvl:]]):\n",
        "            out = f\"{base}_{phase}.tfrecord\"\n",
        "            with tf.io.TFRecordWriter(out) as w:\n",
        "                for r in chunk: w.write(r)\n",
        "            print(f\"  â†’ {out}: {len(chunk)} records\")\n",
        "\n",
        "    # loop datasets\n",
        "    for a in datasets:\n",
        "        src = csv_map[a]\n",
        "        if not os.path.exists(src):\n",
        "            print(f\"âš ï¸ skip {src}: not found\")\n",
        "            continue\n",
        "        # skip if already did\n",
        "        if os.path.exists(f\"{a}_train.tfrecord\") and (a=='parsed_dataset' or os.path.exists(f\"Normal_{a}_train.tfrecord\")):\n",
        "            print(f\"Skipping {a} (already TFRecordâ€™d)\")\n",
        "            continue\n",
        "\n",
        "        print(f\"\\nâŽ¯âŽ¯âŽ¯âŽ¯ {a} âŽ¯âŽ¯âŽ¯âŽ¯\")\n",
        "        df = preprocess_windows(src)\n",
        "        df_att  = df[df.label==1]\n",
        "        df_norm = df[df.label==0]\n",
        "        if len(df_att)>0:\n",
        "            split_and_write(df_att, a)\n",
        "        split_and_write(df_norm, f\"Normal_{a}\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3) Build train/val datasets on Normal windows\n",
        "# ------------------------------------------------------------\n",
        "schema = {\n",
        "    'input_features': tf.io.FixedLenFeature([29*29*2], tf.int64),\n",
        "    'label':          tf.io.FixedLenFeature([1],      tf.int64)\n",
        "}\n",
        "def parse_norm(proto):\n",
        "    p = tf.io.parse_single_example(proto, schema)\n",
        "    x = tf.cast(p['input_features'], tf.float32)\n",
        "    return x, x\n",
        "\n",
        "train_norm = sorted(glob.glob(\"Normal_*_train.tfrecord\"))\n",
        "val_norm   = sorted(glob.glob(\"Normal_*_val.tfrecord\"))\n",
        "\n",
        "# sanity check before training\n",
        "if not train_norm or not val_norm:\n",
        "    raise FileNotFoundError(\"No Normal train/val TFRecords found â€“ cannot train!\")\n",
        "\n",
        "train_ds = (tf.data.TFRecordDataset(train_norm)\n",
        "            .map(parse_norm, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "            .shuffle(10000).batch(64).prefetch(tf.data.AUTOTUNE))\n",
        "val_ds   = (tf.data.TFRecordDataset(val_norm)\n",
        "            .map(parse_norm, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "            .batch(64).prefetch(tf.data.AUTOTUNE))\n",
        "print(f\"\\nâœ… train_ds files: {len(train_norm)}, val_ds files: {len(val_norm)}\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4) Define & train SimpleAE\n",
        "# ------------------------------------------------------------\n",
        "@register_keras_serializable()\n",
        "class SimpleAE(tf.keras.Model):\n",
        "    def __init__(self, input_dim, latent_dim=128, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.input_dim  = input_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.encoder = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(512),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.LeakyReLU(0.2),\n",
        "            tf.keras.layers.Dropout(0.2),\n",
        "            tf.keras.layers.Dense(latent_dim),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.LeakyReLU(0.2),\n",
        "        ])\n",
        "        self.decoder = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(512),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.LeakyReLU(0.2),\n",
        "            tf.keras.layers.Dropout(0.2),\n",
        "            tf.keras.layers.Dense(input_dim),\n",
        "        ])\n",
        "    def call(self, x):\n",
        "        return self.decoder(self.encoder(x))\n",
        "    def get_config(self):\n",
        "        cfg = super().get_config()\n",
        "        cfg.update({'input_dim': self.input_dim, 'latent_dim': self.latent_dim})\n",
        "        return cfg\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n",
        "\n",
        "input_dim = 29*29*2\n",
        "ae = SimpleAE(input_dim=input_dim, latent_dim=128)\n",
        "ae.compile(optimizer=tf.keras.optimizers.Adam(1e-3,0.5,0.9), loss='mse')\n",
        "ae.fit(train_ds, epochs=25, validation_data=val_ds,\n",
        "       callbacks=[tf.keras.callbacks.EarlyStopping(patience=5,restore_best_weights=True)])\n",
        "ae.save('simple_autoencoder.keras')\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 5) Evaluate on Normal & Attack with ROC & Youdenâ€™s J\n",
        "# ------------------------------------------------------------\n",
        "loaded = tf.keras.models.load_model('simple_autoencoder.keras')\n",
        "\n",
        "# normal validation\n",
        "val_ds_norm = (\n",
        "    tf.data.TFRecordDataset(val_norm)\n",
        "      .map(parse_norm, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "      .batch(64)\n",
        ")\n",
        "val_x = np.concatenate([x.numpy() for x,_ in val_ds_norm], axis=0)\n",
        "rec_norm = loaded.predict(val_x, verbose=0)\n",
        "err_norm = np.mean((val_x - rec_norm)**2, axis=1)\n",
        "\n",
        "# attack test\n",
        "test_frs = sorted(glob.glob(\"*_test.tfrecord\"))\n",
        "attack_frs = [f for f in test_frs if not f.startswith(\"Normal_\")]\n",
        "errs = []\n",
        "for af in attack_frs:\n",
        "    ds = (\n",
        "      tf.data.TFRecordDataset([af])\n",
        "        .map(lambda p: tf.cast(tf.io.parse_single_example(p, schema)['input_features'], tf.float32),\n",
        "             num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        .batch(64)\n",
        "    )\n",
        "    for xb in ds:\n",
        "        rec = loaded.predict(xb, verbose=0)\n",
        "        errs.append(np.mean((xb.numpy() - rec)**2, axis=1))\n",
        "err_att = np.concatenate(errs)\n",
        "\n",
        "# ROC + Youdenâ€™s J\n",
        "y_true = np.concatenate([np.zeros_like(err_norm), np.ones_like(err_att)])\n",
        "scores = np.concatenate([err_norm, err_att])\n",
        "\n",
        "fpr, tpr, ths = roc_curve(y_true, scores)\n",
        "roc_auc       = auc(fpr, tpr)\n",
        "opt_i         = np.argmax(tpr - fpr)\n",
        "opt_thresh    = ths[opt_i]\n",
        "\n",
        "print(f\"\\nROC AUC = {roc_auc:.4f}\")\n",
        "print(f\"Youdenâ€™s J optimal thresh = {opt_thresh:.6f}  (TPR={tpr[opt_i]:.3f}, FPR={fpr[opt_i]:.3f})\")\n",
        "\n",
        "y_pred = (scores > opt_thresh).astype(int)\n",
        "print(\"\\nConfusion Matrix @ optimal thresh:\\n\", confusion_matrix(y_true, y_pred, labels=[0,1]))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred, target_names=['Normal','Attack']))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BqrCwNzt9eQ7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}